import torch
import torch.nn as nn
import warnings
import math

def crop_tensor( image_pack : torch.tensor, scale_x : int, scale_y = None, newis = False) -> torch.tensor:
    """
    https://github.com/aoxipo/AITOOTH/model/utils.py
    切分tensor 为大小相同的patch
    以 scale_x 和 scale_y 切分 w 和 h 
    param: image_pack 图像
    param: scale_x 在w上均分为scale_x 部分
    param: scale_y 在h上均分为scale_y 部分
    """
    if scale_y is None:
        scale_y = scale_x
    _, _, w, h = image_pack.size()
    a = int(w/scale_x)
    b = int(h/scale_y)
    # print(a, b)
    t = torch.split(image_pack, a, dim = 2)
    ans = []
    for i in t:
        for j in torch.split(i, b, dim=3):
            ans.append(j)
            # print(j.shape)
    if newis:
        d = torch.stack(ans, 1)
    else:
        d = torch.cat(ans, 0)
    return d

def cat_tensor( image_pack : torch.tensor, scale_x : int, scale_y = None) -> torch.tensor:
    """
    https://github.com/aoxipo/AITOOTH/model/utils.py
    还原之前的切分操作
    param: image_pack 图像
    param: scale_x 在w上均分为scale_x 部分
    param: scale_y 在h上均分为scale_y 部分
    """
    if len(image_pack.shape) == 4:
        B, C, W, H = image_pack.shape
        patchNumber = scale_x * scale_y
        image_pack = image_pack.view( B//patchNumber, patchNumber, C, W, H)
    if scale_y is None:
        scale_y = scale_x
    data = []
    for i in range(scale_x):
        m = []
        for j in range(scale_y):
            m.append(image_pack[:, i * scale_y + j ,:,:,:])
            # print(  i * scale_y + j, i,j )
        data.append(torch.cat(m, dim = -1))
    
    data = torch.cat(data, dim = -2)
    return data

def to_2tuple(x):
    return (x,x)

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution.

    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py

    The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            'mean is more than 2 std from [a, b] in nn.init.trunc_normal_. '
            'The distribution of values may be incorrect.',
            stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        low = norm_cdf((a - mean) / std)
        up = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [low, up], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * low - 1, 2 * up - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py
    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


def window_partition(x, window_size):
    """
    Args:
        x: (b, h, w, c)
        window_size (int): window size

    Returns:
        windows: (num_windows*b, window_size, window_size, c)
    """
    b, h, w, c = x.shape
    x = x.view(b, h // window_size, window_size, w // window_size, window_size, c)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, c)
    return windows

def window_reverse(windows, window_size, h, w):
    """
    Args:
        windows: (num_windows*b, window_size, window_size, c)
        window_size (int): Window size
        h (int): Height of image
        w (int): Width of image

    Returns:
        x: (b, h, w, c)
    """
    b = int(windows.shape[0] / (h * w / window_size / window_size))
    x = windows.view(b, h // window_size, w // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)
    return x

def calculate_rpi_oca( window_size = 16, overlap_ratio = 0.5 ):
    # calculate relative position index for OCA
    window_size_ori = window_size
    window_size_ext = window_size + int(overlap_ratio * window_size)

    coords_h = torch.arange(window_size_ori)
    coords_w = torch.arange(window_size_ori)
    coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, ws, ws
    coords_ori_flatten = torch.flatten(coords_ori, 1)  # 2, ws*ws

    coords_h = torch.arange(window_size_ext)
    coords_w = torch.arange(window_size_ext)
    coords_ext = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, wse, wse
    coords_ext_flatten = torch.flatten(coords_ext, 1)  # 2, wse*wse

    relative_coords = coords_ext_flatten[:, None, :] - coords_ori_flatten[:, :, None]   # 2, ws*ws, wse*wse

    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # ws*ws, wse*wse, 2
    relative_coords[:, :, 0] += window_size_ori - window_size_ext + 1  # shift to start from 0
    relative_coords[:, :, 1] += window_size_ori - window_size_ext + 1

    relative_coords[:, :, 0] *= window_size_ori + window_size_ext - 1
    relative_position_index = relative_coords.sum(-1)
    return relative_position_index